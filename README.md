## Overview

This project implements a reproducible test environment in AWS using Amazon EKS.
The platform allows developers to deploy and test applications on a separate
test domain before exposing them on the production domain.

The solution includes:
- Kubernetes (EKS)
- Automated DNS management via Route53
- HTTPS using Letâ€™s Encrypt with cert-manager
- Secure AWS access using IAM Roles for Service Accounts (IRSA)

ðŸš« cert-manager annotations and TLS configuration are blocked by policy.

All TLS, certificates, and DNS are managed by the platform.
Ingress resources containing cert-manager configuration will be rejected by admission policy.

The platform follows cloud and Kubernetes best practices and is suitable
both for real-world usage and as a foundation for an academic thesis.

## Purpose of this Repository

This repository defines a **reproducible Kubernetes application platform on AWS (EKS)**. It is designed to:

- Provide a **clear contract** between platform engineering and application teams
- Enable **self-service application deployments** via CI/CD
- Separate **stable (frozen) platform infrastructure** from **application-owned configuration**

The repository is intentionally structured so that a future DevOps engineer can take over with minimal context, and so that the same structure can be reused for academic or portfolio purposes.

## Scope & Intentional Limitations

This repository intentionally focuses on a **test environment** architecture.

The following areas are **explicitly out of scope** and intentionally not implemented:

- Production environments
- Secrets management (e.g. AWS Secrets Manager, Vault)
- Multi-cluster or multi-region setups
- Blue/green or canary deployments
- GitOps controllers (e.g. Argo CD, Flux)
- Centralized logging or monitoring stacks

These omissions are deliberate in order to:
- Keep the architecture understandable and auditable
- Focus on platform fundamentals rather than operational scale
- Serve as a reproducible reference for learning, testing, and academic evaluation

## Operational Lifecycle

This repository defines a **controlled application lifecycle** for the shared
EKS test environment.

Applications are expected to follow the lifecycle below.

---

### 1. Develop & Containerize

Application teams:

- Develop frontend and/or backend services
- Build container images locally or via CI
- Push images to Amazon ECR

The repository provides:
- Example `Dockerfile` templates
- Reference Kubernetes manifests under `apps/_template`

---

### 2. Deploy to Test Environment

Deployment to the test cluster is performed via a **manual GitHub Actions workflow**.

- Applications are deployed under the `apps/` namespace
- Traffic is routed through a **shared Application Load Balancer**
- DNS and TLS are managed centrally by the platform

This stage allows teams to:
- Validate functionality
- Perform load testing
- Verify integration between frontend and backend services

---

### 3. Test & Validate

While running in the test environment, teams may:

- Access the application via the test domain
- Perform functional and load testing
- Observe scaling behavior (HPA)
- Validate network and security policies

The test cluster is intentionally **isolated from production**.

---

### 4. Cleanup Test Resources

Once testing is complete, applications **must be removed** from the test cluster.

Cleanup is performed via a dedicated GitHub Actions workflow:

- Requires explicit confirmation (`confirm=DELETE`)
- Removes all Kubernetes resources for the application
- Prevents unused workloads from consuming cluster resources

No cleanup is performed automatically.

---

### 5. Promote to Production (Out of Scope)

Promotion to a production environment is **intentionally out of scope** for this repository.

The test environment exists solely to:
- Validate deployments
- Reduce risk before production
- Provide a controlled and cost-aware testing platform

---

### Lifecycle Summary

```
Develop â†’ Build Image â†’ Deploy to Test â†’ Validate â†’ Cleanup
```

This lifecycle ensures:

- Predictable resource usage
- Clear ownership boundaries
- Safe experimentation without production impact

## Architecture at a Glance

At a high level, the platform is structured around a clear separation of responsibilities:

- **Platform layer**  
  Owns ingress, DNS, TLS, IAM, and cluster-wide policies  
  Managed by the platform / DevOps team

- **Application layer**  
  Owns application code, container images, and Kubernetes manifests  
  Managed by application teams

Applications are deployed into a shared Amazon EKS cluster and exposed via a centrally managed
Application Load Balancer (ALB) using a dedicated test domain.

All security-sensitive infrastructure components are frozen and protected by policy.

## High-Level Architecture

```
Internet
   |
Route53 (public hosted zone)
   |
AWS Application Load Balancer (ALB)
   |
EKS Ingress (AWS Load Balancer Controller)
   |
Kubernetes Services
   |
Application Pods
```

Key integrations:
- **external-dns**: automatically manages Route53 records
- **cert-manager**: manages TLS certificates
- **AWS ACM**: provides certificates used by ALB
- **IRSA**: secure AWS access for controllers

---

## Repository Structure

```
repo/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ deploy-app.yml          # CI/CD pipeline (build â†’ ECR â†’ deploy)
â”‚       â”œâ”€â”€ cleanup-app.yml         # CI/CD cleanup (explicit app removal from test cluster)
        â””â”€â”€ README.md               # CI/CD usage documentation
â”‚
â”œâ”€â”€ platform/                       # owned by platform team (FROZEN)
â”‚   â”œâ”€â”€ ingress.yaml                # shared ALB ingress
â”‚   â”œâ”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ cert-manager/               # TLS infrastructure (FROZEN)
â”‚   â”‚   â”œâ”€â”€ cluster-issuer.yaml
â”‚   â”‚   â”œâ”€â”€ cluster-issuer-staging.yaml
â”‚   â”‚   â”œâ”€â”€ cluster-issuer-dns01-prod.yaml
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ network-policies/           # cluster-level security (FROZEN)
â”‚   â”‚   â”œâ”€â”€ default-deny-apps.yml
â”‚   â”‚   â”œâ”€â”€ default-deny-egress-apps.yml
â”‚   â”‚   â”œâ”€â”€ allow-frontend-to-backend.yml
â”‚   â”‚   â”œâ”€â”€ allow-dns-egress.yml
â”‚   â”‚   â”œâ”€â”€ backend-egress-internet.yml
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â””â”€â”€ policies/                   # admission policies (FROZEN)
â”‚       â”œâ”€â”€ block-cert-manager-ingress.yml
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ apps/                           # owned by application teams
â”‚   â”œâ”€â”€ _template/                  # reference template (never deployed)
â”‚   â”‚   â”œâ”€â”€ deployment.yml
â”‚   â”‚   â”œâ”€â”€ service.yml
â”‚   â”‚   â”œâ”€â”€ ingress.yml
â”‚   â”‚   â”œâ”€â”€ Dockerfile.example.dockerfile
â”‚   â”‚   â”œâ”€â”€ .env.example.yml
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ hello/                      # example single-service app
â”‚   â”‚   â”œâ”€â”€ deployment.yml
â”‚   â”‚   â”œâ”€â”€ service.yml
â”‚   â”‚   â”œâ”€â”€ ingress.yml
â”‚   â”‚   â”œâ”€â”€ Dockerfile.example.dockerfile
â”‚   â”‚   â”œâ”€â”€ .env.example.yml
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â””â”€â”€ example-fullstack/          # frontend + backend reference app
â”‚       â”œâ”€â”€ .env.example.md
â”‚       â”œâ”€â”€ ingress.yml
â”‚       â”œâ”€â”€ network-policy.yml
â”‚       â”œâ”€â”€ README.md
â”‚       â”‚
â”‚       â”œâ”€â”€ frontend/
â”‚       â”‚   â”œâ”€â”€ deployment.yml
â”‚       â”‚   â”œâ”€â”€ service.yml
â”‚       â”‚   â””â”€â”€ Dockerfile.example.dockerfile
â”‚       â”‚
â”‚       â””â”€â”€ backend/
â”‚           â”œâ”€â”€ deployment.yml
â”‚           â”œâ”€â”€ service.yml
â”‚           â”œâ”€â”€ hpa.yml
â”‚           â””â”€â”€ Dockerfile.example.dockerfile
â”‚
â”œâ”€â”€ iam/                            # AWS IAM policies (IRSA)
â”‚   â”œâ”€â”€ aws-load-balancer-controller-policy.json
â”‚   â”œâ”€â”€ cert-manager-dns01-policy.json
â”‚   â”œâ”€â”€ external-dns-policy.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md             # ASCII architecture diagram
â”‚   â”œâ”€â”€ eks.md
â”‚   â”œâ”€â”€ ingress.md
â”‚   â”œâ”€â”€ security.md
â”‚   â””â”€â”€ troubleshooting.md
â”‚
â””â”€â”€ README.md                       # main documentation (this file)

```

---

## ðŸ”’ Frozen Components (DO NOT MODIFY)

The following components are considered **platform-level infrastructure**. They are versioned, stable, and must not be modified by application teams.

### 1. Shared ALB Configuration

**Location:** Kubernetes Ingress resources using shared ALB annotations

This functionality:
- Uses a shared, internet-facing AWS Application Load Balancer
- Is managed by the AWS Load Balancer Controller
- Relies on standardized annotations for listeners, target type, and grouping
- Acts as the common entry point for all applications in the test environment

Why frozen:
- Changing this affects **all applications**
- Must be coordinated with DNS, certificates, and AWS networking

---

### 2. ClusterIssuers (cert-manager)

**Location:** `platform/cert-manager/`

Defines:
- Letâ€™s Encrypt staging issuer
- Letâ€™s Encrypt production issuer

Why frozen:
- Central trust configuration
- Incorrect changes may break certificate issuance cluster-wide

---

### 3. IAM Policies (IRSA)

**Location:** `platform/iam/`

Used by:
- AWS Load Balancer Controller
- external-dns
- cert-manager

Why frozen:
- Security-critical
- Changes require AWS review and testing

---

### 4. Network Policies

**Location:** `platform/network-policies/`

Defines:
- Default-deny ingress and egress policies
- Explicit service-to-service communication rules

Why frozen:
- Enforces cluster-wide zero-trust networking
- Prevents accidental exposure between applications
- Must remain consistent across all deployments

The `apps` namespace uses a default-deny NetworkPolicy.

All pod-to-pod traffic is denied unless explicitly allowed.
Each application must define its required communication paths.

This reduces attack surface and enforces service isolation by default.

---

## ðŸ”“ Application-Owned Configuration

Application teams are expected to manage everything under `apps/<app-name>/`.

### Required Files Per Application

#### `deployment.yaml`
- Defines pods, containers, images, resources

#### `service.yaml`
- Exposes the application internally in the cluster

#### `ingress.yaml`
- Defines the public hostname
- Connects the app to the shared ALB
- References the TLS certificate via annotation

Example responsibilities:
- Hostname (`app.test.example.com`)
- Path routing
- Backend service mapping

Application teams **must not**:
- Create ALBs
- Manage certificates directly
- Modify Route53 records manually

---

## DNS & TLS Flow (DNS-01 Validation)

1. Application ingress is applied
2. AWS Load Balancer Controller updates ALB
3. external-dns creates/updates Route53 record
4. ACM certificate is attached to ALB listener
5. HTTPS traffic is terminated at ALB

---

## CI/CD Expectations

A typical pipeline for an application should:

1. Build container image
2. Push image to registry
3. Apply manifests from `apps/<app-name>/`

The platform layer is deployed separately and infrequently.

CI/CD pipelines are intentionally limited to application-owned resources.

Platform-level components are not modified through application pipelines.

---

## Design Principles

- **Separation of concerns**: platform vs application
- **Least privilege**: IRSA everywhere
- **Minimal YAML per app**
- **Explicit ownership** via directory structure

---

## Status

- Platform components: **FROZEN**
- Application onboarding: **ACTIVE**
- Documentation: **IN PROGRESS**

---

## Notes for Future Maintainers

This environment represents a **shared test / pre-production domain**.

Frozen components are considered stable **within the scope of this test environment**, but *may still evolve* as part of controlled platform work.

If changes are required to frozen components:

1. Document the motivation (what problem is being solved)
2. Apply changes in a dedicated feature branch
3. Validate changes in this test domain before promoting patterns to production

The intent of freezing is to prevent **uncoordinated or accidental changes**, not to block deliberate platform evolution.

---

## Developer Workflow (Frontend & Backend)

Application teams are expected to build and own their container images.

A typical workflow:

### 1. Build Docker Images

Each application (frontend or backend) should provide its own `Dockerfile`.

Example:

```bash
docker build -t <registry>/<app-name>:<tag> .
docker push <registry>/<app-name>:<tag>
```

Images should:
- be immutable (tagged via commit SHA or version)
- expose the correct container port
- not contain environment-specific configuration

---

### 2. Deploy to the Test Domain

Once an image is published:

1. Update `deployment.yaml` with the new image tag
2. Apply manifests under `apps/<app-name>/`

```bash
kubectl apply -f apps/<app-name>/
```

Frontend and backend services should:
- use **separate Deployments and Services**
- use **distinct hostnames** or paths as agreed (e.g. `api.test.example.com` and `app.test.example.com`)

---

### 3. Validate

After deployment:
- verify DNS resolution
- verify HTTPS access
- validate service-to-service communication if applicable

---

## Application Lifecycle and Cleanup

Applications deployed to the test environment are **temporary by design**.

The test domain is used exclusively to validate application behavior before
promotion to the production domain.

Once an application has been verified:

- All Kubernetes resources for the application **must be removed**
- No pods or services should remain running in the test cluster
- This prevents unnecessary cloud costs and resource sprawl

### Cleanup process

Applications are removed using a dedicated CI/CD cleanup workflow.

This workflow:
- Deletes all application-owned Kubernetes manifests
- Automatically removes ALB rules and DNS records
- Ensures the test cluster returns to an idle state

The test environment is therefore treated as **ephemeral infrastructure**,
not a long-lived staging platform.


---

**This workflow enables teams to independently iterate on applications while relying on a stable shared platform.**

---

## Intended Audience

This repository is intended for:

- Platform and DevOps engineers maintaining shared Kubernetes infrastructure
- Application developers deploying services into a managed test environment
- Academic reviewers evaluating cloud-native platform design
- Future maintainers taking over the platform after initial development

The repository prioritizes clarity, documentation, and explicit boundaries over feature completeness.
